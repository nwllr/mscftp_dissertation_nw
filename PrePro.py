import pandas as pdimport numpy as npfrom statistics import modefrom scipy import statsfrom sklearn.preprocessing import MinMaxScaler, StandardScalerdef read_clean_dataset():    """    Reads the cleaned dataset from a CSV file and preprocesses it.    The dataset is sorted by 'Date', and the 'Date' column is converted to datetime period of format 'M'.    The index of the DataFrame is set to ['Date', 'tic'].    Returns:        pd.DataFrame: The preprocessed dataset.    """       dataset_raw = pd.read_csv('data/cleaned_dataset_2M_delay.csv')        dataset = dataset_raw.sort_values('Date', inplace=False)    dataset['Date'] = pd.to_datetime(dataset['Date']).dt.to_period('M')    dataset.set_index(['Date', 'tic'], inplace=True)    return datasetdef prepare_features_and_target(dataset):    """    Prepares features and target from the input dataset.    One hot encoding is applied to the dataset.    'Company Market Cap' is used as the target, the rest columns are used as features.    Args:        dataset (pd.DataFrame): The input dataset.    Returns:        Tuple[pd.DataFrame, pd.Series]: The features DataFrame and the target Series.    """        # dataset_onehot = dataset.drop(columns=['Date', 'tic'])    dataset_onehot = pd.get_dummies(dataset)        ## Define Target and Features        target = dataset_onehot['Company Market Cap']    features = dataset_onehot.drop(columns=['Company Market Cap'])        return features, target## Train, Test, Validation Splitdef train_val_test_split(features, target, val_from=2017, tst_from=2019):    """    Splits the features and the target into training, validation, and testing sets.    The splitting is done based on the year from the 'Date' index.    Args:        features (pd.DataFrame): The features.        target (pd.Series): The target.        val_from (int, optional): The year to start the validation set. Default is 2017.        tst_from (int, optional): The year to start the test set. Default is 2019.    Returns:        Tuple: The full training, training, validation, and testing sets of the features and the target.    """        assert features.shape[0] == len(target)        # Split the features    Xtrn_full = features[features.index.get_level_values('Date').year < tst_from] # about 72.5% of the data    Xtst = features[features.index.get_level_values('Date').year >= tst_from] # about 27.5% of the data    Xtrn = Xtrn_full[Xtrn_full.index.get_level_values('Date').year < val_from] # about 60% of the data    Xval = Xtrn_full[Xtrn_full.index.get_level_values('Date').year >= val_from] # about 12.6% of the data and 17.3% of the full training set        # Split the target accordingly    Ytrn_full = target[target.index.get_level_values('Date').year < tst_from] # about 72.5% of the data    Ytst = target[target.index.get_level_values('Date').year >= tst_from] # about 27.5% of the data    Ytrn = Ytrn_full[Ytrn_full.index.get_level_values('Date').year < val_from] # about 60% of the data    Yval = Ytrn_full[Ytrn_full.index.get_level_values('Date').year >= val_from] # about 12.6% of the data and 17.3% of the full training set    return Xtrn_full, Xtrn, Xval, Xtst, Ytrn_full, Ytrn, Yval, Ytst## Scaling the featuresdef scale_minmax(Xtrn_full, Xtrn, Xval, Xtst):    """    Scales the input datasets using MinMaxScaler.    The scaler is fitted on the training set, and the scaling is applied to all sets.    Args:        Xtrn_full (pd.DataFrame): The full training set.        Xtrn (pd.DataFrame): The training set.        Xval (pd.DataFrame): The validation set.        Xtst (pd.DataFrame): The testing set.    Returns:        Tuple: The scaled full training, training, validation, and testing sets.    """        minmax = MinMaxScaler()    Xtrn_minmax = minmax.fit_transform(Xtrn)        Xtrn_full_minmax = minmax.transform(Xtrn_full)    Xval_minmax = minmax.transform(Xval)    Xtst_minmax = minmax.transform(Xtst)        return Xtrn_full_minmax, Xtrn_minmax, Xval_minmax, Xtst_minmaxdef scale_standard(Xtrn_full, Xtrn, Xval, Xtst):    """    Scales the input datasets using StandardScaler.    The scaler is fitted on the training set, and the scaling is applied to all sets.    Args:        Xtrn_full (pd.DataFrame): The full training set.        Xtrn (pd.DataFrame): The training set.        Xval (pd.DataFrame): The validation set.        Xtst (pd.DataFrame): The testing set.    Returns:        Tuple: The scaled full training, training, validation, and testing sets.    """        stand_scaler = StandardScaler()    Xtrn_stand = stand_scaler.fit_transform(Xtrn)        Xtrn_full_stand = stand_scaler.transform(Xtrn_full)    Xval_stand = stand_scaler.transform(Xval)    Xtst_stand = stand_scaler.transform(Xtst)        return Xtrn_full_stand, Xtrn_stand, Xval_stand, Xtst_stand    def winsorize_dataframe(df, lower_limit=0.05, upper_limit=0.95):    """    Winsorizes the numerical features in the DataFrame.    The winsorizing is done with the given limits, and the boolean features are kept unchanged.    Args:        df (pd.DataFrame): The DataFrame to be winsorized.        lower_limit (float, optional): The lower limit for winsorizing. Default is 0.01.        upper_limit (float, optional): The upper limit for winsorizing. Default is 0.99.    Returns:        pd.DataFrame: The winsorized DataFrame.    """    numerical_features = df.select_dtypes(include=[np.number])    boolean_features = df.select_dtypes(include=[bool])    numerical_features = numerical_features.apply(lambda x: stats.mstats.winsorize(x, limits=(lower_limit, 1-upper_limit)))    return pd.concat([numerical_features, boolean_features], axis=1)def winsorize_and_scale(Xtrn_full, Xtrn, Xval, Xtst):    """    Winsorizes and scales the input datasets.    Winsorizing is done first, then MinMax scaling is applied.    Args:        Xtrn_full (pd.DataFrame): The full training set.        Xtrn (pd.DataFrame): The training set.        Xval (pd.DataFrame): The validation set.        Xtst (pd.DataFrame): The testing set.    Returns:        Tuple: The winsorized and scaled full training, training, validation, and testing sets.    """    # Winsorizing the features    Xtrn_winsor = winsorize_dataframe(Xtrn)    Xval_winsor = winsorize_dataframe(Xval)    Xtst_winsor = winsorize_dataframe(Xtst)    Xtrn_full_winsor = winsorize_dataframe(Xtrn_full)        # Now scale the winsorized data and return it    return scale_minmax(Xtrn_full_winsor, Xtrn_winsor, Xval_winsor, Xtst_winsor)## log-transforming the targetdef log_transform_target(Ytrn_full, Ytrn, Yval, Ytst):    """    Applies log transformation to the target sets.    A constant of 1 is added to handle zeros before the transformation.    Args:        Ytrn_full (pd.Series): The full training set target.        Ytrn (pd.Series): The training set target.        Yval (pd.Series): The validation set target.        Ytst (pd.Series): The testing set target.    Returns:        List[pd.Series]: The log transformed full training, training, validation, and testing targets.    """    target_list = [Ytrn_full, Ytrn, Yval, Ytst]    target_list_log = []        for y in target_list:        # Add a small constant (e.g., 1) to the target variable to handle zeros (if necessary)        y_log = y + 1                # Apply the natural logarithm transformation to the target variable        y_log = np.log(y_log)                target_list_log.append(y_log)                   return target_list_log                    